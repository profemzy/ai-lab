# Copy this file to ".env" and adjust values as needed.
# The app auto-loads .env (without overriding existing environment variables).
# Precedence: CLI > environment vars (.env) > labs.toml > built-in defaults.

# Core model selection
LABS_MODEL=Qwen/Qwen2.5-7B-Instruct

# Generation params
LABS_MAX_NEW_TOKENS=128
LABS_TEMPERATURE=0.7
LABS_TOP_P=0.9
# LABS_TOP_K=50
LABS_DO_SAMPLE=true
# LABS_REPETITION_PENALTY=1.1

# Runtime / device
# Use "auto" to let Accelerate place across available devices. Use "cuda" to force a single GPU,
# or "cpu" to force CPU. For multi-GPU, leave "auto" unless you have a specific policy.
LABS_DEVICE_MAP=auto

# Dtype preferences: bf16 | fp16 | fp32
# Prefer bf16 on modern NVIDIA GPUs (Ampere+). Use fp16 if bf16 is unavailable.
LABS_TORCH_DTYPE=bf16

# trust_remote_code allows custom model code execution from the repo (use only with trusted sources)
LABS_TRUST_REMOTE_CODE=false

# Chat/template toggles
LABS_USE_CHAT_TEMPLATE=true
LABS_ADD_GENERATION_PROMPT=true

# Quantization (optional; enable at most one)
LABS_LOAD_IN_4BIT=false
LABS_LOAD_IN_8BIT=false

# 4-bit quantization options (only used if LABS_LOAD_IN_4BIT=true)
# LABS_BNB_4BIT_COMPUTE_DTYPE=bf16   # or fp16
# LABS_BNB_4BIT_QUANT_TYPE=nf4
# LABS_BNB_4BIT_USE_DOUBLE_QUANT=true

# Optional: point to a non-default config file (labs.toml)
# LABS_CONFIG=./labs.toml